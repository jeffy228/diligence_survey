<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DiligenceSquared Screener + Interview</title>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;600&display=swap');
    :root {
      --bg: #0f172a;
      --panel: #111827;
      --accent: #0ea5e9;
      --accent-2: #f59e0b;
      --text: #e5e7eb;
      --muted: #9ca3af;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: 'Space Grotesk', 'Helvetica Neue', Arial, sans-serif;
      background: radial-gradient(circle at 10% 20%, rgba(14,165,233,0.12), transparent 30%),
                  radial-gradient(circle at 90% 10%, rgba(245,158,11,0.12), transparent 28%),
                  var(--bg);
      color: var(--text);
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 32px 16px 64px;
    }
    h1 { margin: 0 0 8px; letter-spacing: -0.02em; }
    p { margin: 6px 0; color: var(--muted); }
    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
      gap: 16px;
      width: 100%;
      max-width: 1200px;
      margin-top: 24px;
    }
    .panel {
      background: linear-gradient(145deg, rgba(17,24,39,0.9), rgba(17,24,39,0.75));
      border: 1px solid rgba(255,255,255,0.08);
      border-radius: 14px;
      padding: 18px;
      box-shadow: 0 20px 40px rgba(0,0,0,0.35);
    }
    .status {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 12px;
      border-radius: 10px;
      background: rgba(255,255,255,0.05);
      border: 1px solid rgba(255,255,255,0.08);
      margin: 12px 0;
    }
    .pill {
      background: var(--accent);
      color: #0b0f19;
      font-weight: 700;
      padding: 4px 10px;
      border-radius: 999px;
      font-size: 12px;
      text-transform: uppercase;
      letter-spacing: 0.04em;
    }
    button {
      border: none;
      border-radius: 10px;
      padding: 10px 14px;
      background: var(--accent);
      color: #0b0f19;
      font-weight: 700;
      cursor: pointer;
      transition: transform 120ms ease, box-shadow 120ms ease, opacity 120ms ease;
      box-shadow: 0 10px 30px rgba(14,165,233,0.25);
    }
    button:hover { transform: translateY(-1px); }
    button:disabled { opacity: 0.5; cursor: not-allowed; transform: none; box-shadow: none; }
    input, .mono {
      width: 100%;
      border: 1px solid rgba(255,255,255,0.12);
      background: rgba(255,255,255,0.04);
      border-radius: 10px;
      padding: 10px 12px;
      color: var(--text);
      font-family: 'Space Grotesk', monospace;
    }
    iframe {
      width: 100%;
      min-height: 520px;
      border: none;
      border-radius: 12px;
      background: #0b0f19;
    }
    .row {
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
      align-items: center;
    }
    .divider {
      height: 1px;
      background: rgba(255,255,255,0.08);
      margin: 12px 0;
    }
    .tag { color: var(--muted); font-size: 12px; text-transform: uppercase; letter-spacing: 0.05em; }
    ul { padding-left: 18px; margin: 6px 0; color: var(--muted); }
    code { background: rgba(255,255,255,0.06); padding: 2px 6px; border-radius: 6px; }
  </style>
</head>
<body>
  <header style="width:100%; max-width:960px;">
    <h1>BMW Screener ➜ Voice Interview</h1>
    <p>Answer the screener below, then start the brief voice interview. We'll handle the rest.</p>
    <div class="status">
      <span class="pill" id="state-pill">Pending</span>
      <span id="state-detail">Setting up…</span>
    </div>
  </header>

  <div class="grid" style="max-width:960px;">
    <div class="panel">
      <div class="tag">Step 1 · Screener</div>
      <p>Complete the short survey. Your unique link keeps your progress.</p>
      <iframe id="tf-frame" title="Typeform Screener" allow="microphone; camera; autoplay" style="background:#ffffff;"></iframe>
    </div>

    <div class="panel">
      <div class="tag">Step 2 · Voice Interview</div>
      <p id="segment-label">Waiting for qualification…</p>
      <div class="row">
        <button id="start-interview" disabled>Start Interview</button>
        <button id="pause-interview" disabled>Pause Interview</button>  <button id="end-interview" disabled>Complete Interview</button>
      </div>
      <p id="interview-status" class="mono" style="margin-top:10px;">No conversation yet.</p>
      <div class="divider"></div>
      <p class="tag">Transcript</p>
      <p class="mono" id="transcript-log">You’ll see the conversation here once the interview starts.</p>
    </div>
  </div>

  <script>
    const API_BASE = "https://adjectively-intersubjective-dorothy.ngrok-free.dev";
    const TYPEFORM_FORM_ID = "Agmp3YRN"; // replace with your Typeform ID

    const statePill = document.getElementById("state-pill");
    const stateDetail = document.getElementById("state-detail");
    const tfFrame = document.getElementById("tf-frame");
    const startBtn = document.getElementById("start-interview");
    const endBtn = document.getElementById("end-interview");
    const pauseBtn = document.getElementById("pause-interview");
    const interviewStatus = document.getElementById("interview-status");
    const segmentLabel = document.getElementById("segment-label");
    const transcriptLog = document.getElementById("transcript-log");

    const getQueryParam = (key) => new URLSearchParams(window.location.search).get(key);
    let respondentId = getQueryParam("respondent_id") || getQueryParam("user_id") || localStorage.getItem("respondent_id") || "";
    let lastSession = null;
    let micStream = null;
    let audioCtx = null;
    let processor = null;
    let ws = null;
    let wsUrl = "";
    let sessionContext = {};
    let pollTimer = null;
    let sendAudioAsJson = true; // ElevenLabs ConvAI expects JSON messages with base64 audio
    let outputFormat = "pcm_16000";
    let audioQueue = [];
    let isPlayingAudio = false;
    let interviewEnded = false;
    let conversationHistory = [];

    const setPill = (text, color) => {
      statePill.textContent = text;
      statePill.style.background = color;
    };

    let typeformReadyPromise = null;
    const setTypeformSrc = (id) => {
      if (!tfFrame) return;
      if (["screened_out", "qualified", "interview_in_progress", "interview_complete"].includes(lastSession?.status)) {
        tfFrame.srcdoc = `
          <div style="padding:32px; font-family:Space Grotesk, sans-serif; color:#e5e7eb; background:#0f172a; min-height:520px; display:flex; align-items:center; justify-content:center; text-align:center; border-radius:12px;">
            <div>
              <p style="margin:0 0 8px; font-size:18px; font-weight:600;">Survey not available</p>
              <p style="margin:0; color:#cbd5f5;">${
                lastSession?.status === "screened_out"
                  ? "You did not qualify for this survey. Thanks for your time."
                  : "You’ve already completed screening. Please continue to the interview."
              }</p>
            </div>
          </div>`;
        tfFrame.removeAttribute("src");
        return;
      }
      if (!TYPEFORM_FORM_ID || TYPEFORM_FORM_ID === "YOUR_FORM_ID") {
        tfFrame.srcdoc = "<div style='padding:24px;color:#e5e7eb;font-family:Space Grotesk;'>Set TYPEFORM_FORM_ID in web/index.html to load your screener.</div>";
        return;
      }
      const base = `https://form.typeform.com/to/${TYPEFORM_FORM_ID}`;
      const url = `${base}?typeform-embed=embed-widget&respondent_id=${encodeURIComponent(id)}`;
      tfFrame.removeAttribute("srcdoc");
      tfFrame.src = url;
    };

    const updateUrlWithId = () => {
      if (!respondentId) return;
      const url = new URL(window.location.href);
      url.searchParams.set("respondent_id", respondentId);
      window.history.replaceState({}, "", url.toString());
    };

    async function createOrResume(id) {
      const url = id ? `${API_BASE}/api/session?respondent_id=${encodeURIComponent(id)}` : `${API_BASE}/api/session`;
      const res = await fetch(url, { method: "POST" });
      if (!res.ok) throw new Error("Failed to create/resume session");
      const data = await res.json();
      respondentId = data.respondent_id;
      localStorage.setItem("respondent_id", respondentId);
      lastSession = data.state;
      hydrateUI();
      setTypeformSrc(respondentId);
      updateUrlWithId();
      startSessionPolling();
    }

    async function refreshSession() {
      if (!respondentId) return;
      const res = await fetch(`${API_BASE}/api/session/${encodeURIComponent(respondentId)}`);
      if (!res.ok) return;
      lastSession = await res.json();
      hydrateUI();
    }

    function hydrateUI() {
      if (!respondentId) return;
      const status = lastSession?.status || "pending_screening";
      stateDetail.textContent = status;
      if (status === "interview_complete") setPill("Complete", "#22c55e");
      else if (status === "interview_in_progress") setPill("Interview", "#0ea5e9");
      else if (status === "qualified") setPill("Qualified", "#f59e0b");
      else setPill("Pending", "#6366f1");

      const segment = lastSession?.segment;
      segmentLabel.textContent = segment ? `Segment: ${segment}` : "Waiting for qualification…";
      startBtn.disabled = !(segment && status !== "interview_complete");
      const isInProgress = status === "interview_in_progress";
      pauseBtn.disabled = !isInProgress;
      endBtn.disabled = !isInProgress;
      // pauseBtn.disabled = !isInProgress;
      // endBtn.disabled = !(status === "interview_in_progress");
      interviewStatus.textContent = lastSession?.conversation_id
        ? `Conversation: ${lastSession.conversation_id}`
        : "No conversation yet.";

      if (lastSession?.history && lastSession.history.length) {
        conversationHistory = lastSession.history;
        transcriptLog.textContent = conversationHistory.map(h => `${h.role}: ${h.message}`).join("\n");
      }

      // keep polling until interview is complete so webhook updates are picked up automatically
      if (status === "interview_complete") {
        clearSessionPolling();
      } else {
        startSessionPolling();
      }
    }

    async function startConversation() {
      if (!respondentId) return alert("No respondent");
      const res = await fetch(`${API_BASE}/api/conversation/start`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ respondent_id: respondentId, transcript: conversationHistory }),
      });
      if (!res.ok) {
        const err = await res.text();
        alert("Could not start conversation: " + err);
        return;
      }
      const data = await res.json();
      wsUrl = data.ws_url;
      sessionContext = data.context || {};
      lastSession = { ...(lastSession || {}), conversation_id: data.conversation_id, segment: data.segment, progress: data.progress };
      interviewEnded = false;
      if (lastSession?.history?.length) {
        conversationHistory = lastSession.history;
      } else {
        conversationHistory = [];
      }
      hydrateUI();
      transcriptLog.textContent = "Connecting to ElevenLabs…";
      await connectVoice();
    }

    async function pauseInterview() {
  interviewEnded = true;
  sendEndSignal();
  cleanupVoice();
  if (respondentId) {
    try {
      // Call the new backend /pause endpoint to set status to 'qualified'
      await fetch(`${API_BASE}/api/conversation/pause`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ respondent_id: respondentId }),
      });
      await refreshSession(); // Fetches new status ('qualified') to re-enable 'Start' button
    } catch (err) {
      console.error("pauseInterview error", err);
    }
  }
  transcriptLog.textContent = (transcriptLog.textContent || "") + "\n[Interview paused]";
}

    async function endInterview() {
      interviewEnded = true;
      sendEndSignal();
      cleanupVoice();
      if (respondentId) {
        try {
          await fetch(`${API_BASE}/api/conversation/complete`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ respondent_id: respondentId, transcript: conversationHistory }),
          });
          await refreshSession();
        } catch (err) {
          console.error("endInterview error", err);
        }
      }
      transcriptLog.textContent = (transcriptLog.textContent || "") + "\n[Interview ended]";
    }

    startBtn.onclick = () => startConversation();
    pauseBtn.onclick = () => pauseInterview();
    endBtn.onclick = () => endInterview();

    // init
    (async () => {
      if (respondentId) {
        await createOrResume(respondentId).catch(() => {});
      } else {
        await createOrResume("").catch(() => {});
      }
    })();

    function startSessionPolling() {
      clearSessionPolling();
      pollTimer = setInterval(() => {
        if (respondentId) refreshSession();
      }, 4000);
    }

    function clearSessionPolling() {
      if (pollTimer) {
        clearInterval(pollTimer);
        pollTimer = null;
      }
    }

    // ---- Voice (ElevenLabs) helpers ----
    async function connectVoice() {
      if (!wsUrl) {
        transcriptLog.textContent = "No ElevenLabs ws_url returned.";
        return;
      }
      cleanupVoice();
      try {
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      } catch (err) {
        alert("Microphone permission needed: " + err);
        return;
      }

      ws = new WebSocket(wsUrl);
      ws.binaryType = "arraybuffer";

      ws.onopen = () => {
        transcriptLog.textContent = "Voice link opened. Start speaking…";
        sendConvAIInit();
        startRecording();
      };

      ws.onmessage = (event) => {
        if (typeof event.data === "string") {
          try {
            const msg = JSON.parse(event.data);
            handleConvAIMessage(msg);
          } catch (_) {
            // ignore non-JSON text payloads
          }
        } else {
          enqueueAudio(event.data, "audio/webm");
        }
      };

      ws.onerror = () => {
        transcriptLog.textContent = "Voice link error. Check ElevenLabs creds / ws URL.";
      };

      ws.onclose = () => {
        transcriptLog.textContent = "Voice link closed.";
        stopRecording();
      };
    }

    // need to pass upon creation of websocket

    // when websocket open, the interview's already begun

    // function sendConvAIInit() {
    //   if (!ws || ws.readyState !== WebSocket.OPEN) return;
    //   const c = sessionContext || {};
    //   const initMsg = {
    //     type: "conversation_initiation_client_data",
    //     dynamic_variables: {
    //       respondent_id: respondentId,
    //       segment: c.segment || lastSession?.segment,
    //       summary: c.summary || "",
    //     },
    //   };
    //   ws.send(JSON.stringify(initMsg));
    // }


//     function sendConvAIInit() {
//   if (!ws || ws.readyState !== WebSocket.OPEN) return;
//   const c = sessionContext || {};
  
//   // 1. Transform your local history format (role: 'agent'/'you', message: '...')
//   const historyPayload = conversationHistory.map(h => ({
//     role: h.role === 'agent' ? 'assistant' : 'user',
//     content: h.message
//   }));
  
//   // 2. Determine if this is a resumption and create an acknowledgement message
//   let resumptionMsg = "";
//   if (historyPayload.length > 0) {
//     // Get the last message to summarize where they left off
//     const lastEntry = conversationHistory[conversationHistory.length - 1];
    
//     // Create the message the agent will acknowledge in its first turn
//     resumptionMsg = `It looks like we were just discussing: ${lastEntry.message.substring(0, 80)}...`;
//   }

//   const initMsg = {
//     type: "conversation_initiation_client_data",
//     dynamic_variables: {
//       respondent_id: respondentId,
//       segment: c.segment || lastSession?.segment,
//       summary: c.summary || "",
//       // ⭐ NEW: Pass the acknowledgement message ⭐
//       // resumption_message: resumptionMsg, 
//     },
//     // 3. CRITICAL: Pass the full, structured history in the dedicated field.
//     // This is required for the LLM's memory/context window, even if the dynamic variable 
//     // is used to trigger the verbal acknowledgement.
//     conversation_history: historyPayload, 
//   };
  
//   ws.send(JSON.stringify(initMsg));
// }

    function sendConvAIInit() {
  if (!ws || ws.readyState !== WebSocket.OPEN) return;
  const c = sessionContext || {};

  // 1. Transform your local history format (role: 'agent'/'you', message: '...')
  //    to the standard LLM API format (role: 'assistant'/'user', content: '...')
    const historyPayload = conversationHistory.map(h => ({
      // Map 'agent' to 'assistant' and 'you' to 'user'
      role: h.role === 'agent' ? 'assistant' : 'user',
      content: h.message
    }));

    const initMsg = {
      type: "conversation_initiation_client_data",
      dynamic_variables: {
        respondent_id: respondentId,
        segment: c.segment || lastSession?.segment,
        summary: c.summary || "",
        user_id: respondentId,
      },
      conversation_history: historyPayload,
    };
  
  ws.send(JSON.stringify(initMsg));
}

    function sendEndSignal() {
      if (!ws || ws.readyState !== WebSocket.OPEN) return;
      ws.send(JSON.stringify({ type: "client_message", message: "Interview complete. You can end the call." }));
      try {
        ws.close();
      } catch (_) {}
    }

    function handleConvAIMessage(msg) {
      if (msg.type === "agent_response" && msg.agent_response_event?.agent_response) {
        appendTranscript("agent", msg.agent_response_event.agent_response);
      }
      if (msg.type === "user_transcript" && msg.user_transcription_event?.user_transcript) {
        appendTranscript("you", msg.user_transcription_event.user_transcript);
      }
      if (msg.type === "audio" && msg.audio_event?.audio_base_64) {
        playAudioBase64(msg.audio_event.audio_base_64);
      }
      if (msg.type === "conversation_initiation_metadata" && msg.conversation_initiation_metadata_event) {
        const fmt = msg.conversation_initiation_metadata_event.agent_output_audio_format;
        if (fmt) outputFormat = fmt;
      }
      if (msg.type === "ping") {
        ws?.send(JSON.stringify({ type: "pong", event_id: msg.ping_event?.event_id }));
      }
    }


    function appendTranscript(role, text) {
      const existing = transcriptLog.textContent || "";
      const line = `${role}: ${text}`;
      transcriptLog.textContent = existing ? `${existing}\n${line}` : line;
      conversationHistory.push({ role, message: text });

      // Auto-save the conversation history every time the transcript updates (agent turns)
      if (role === "agent" && respondentId) {
        fetch(`${API_BASE}/api/conversation/autosave`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ respondent_id: respondentId, transcript: conversationHistory }),
        }).catch(err => console.error("Auto-save failed:", err));
      }
    }

    function cleanupVoice() {
      if (ws) ws.close();
      ws = null;
      stopRecording();
      if (micStream) {
        micStream.getTracks().forEach((t) => t.stop());
        micStream = null;
      }
      if (processor) {
        processor.disconnect();
        processor = null;
      }
      if (audioCtx) {
        audioCtx.close();
        audioCtx = null;
      }
    }

    function startRecording() {
      if (!micStream) return;
      const sampleRate = 16000;
      audioCtx = new AudioContext({ sampleRate });
      const source = audioCtx.createMediaStreamSource(micStream);

      // ScriptProcessor is deprecated but works broadly; AudioWorklet would be better with a worklet script.
      processor = audioCtx.createScriptProcessor(4096, 1, 1);
      processor.onaudioprocess = (e) => {
        if (!ws || ws.readyState !== WebSocket.OPEN) return;
        const input = e.inputBuffer.getChannelData(0);
        const pcm = floatTo16BitPCM(input);
        const b64 = btoa(String.fromCharCode.apply(null, pcm));
        ws.send(JSON.stringify({ user_audio_chunk: b64 }));
      };

      source.connect(processor);
      processor.connect(audioCtx.destination);
    }

    function stopRecording() {
      if (processor) {
        processor.disconnect();
        processor = null;
      }
      if (audioCtx) {
        audioCtx.close();
        audioCtx = null;
      }
    }

    function playAudioBase64(b64) {
      const byteChars = atob(b64);
      const bytes = new Uint8Array(byteChars.length);
      for (let i = 0; i < byteChars.length; i++) bytes[i] = byteChars.charCodeAt(i);

      if (outputFormat.startsWith("pcm_")) {
        const sr = Number(outputFormat.split("_")[1]) || 16000;
        const wav = pcm16ToWav(bytes.buffer, sr);
        enqueueAudio(wav, "audio/wav");
      } else {
        enqueueAudio(bytes.buffer, "audio/webm");
      }
    }

    function enqueueAudio(data, mime = "audio/webm") {
      audioQueue.push({ data, mime });
      if (!isPlayingAudio) playNextAudio();
    }

    function playNextAudio() {
      if (!audioQueue.length) {
        isPlayingAudio = false;
        return;
      }
      isPlayingAudio = true;
      const { data, mime } = audioQueue.shift();
      const blob = data instanceof ArrayBuffer ? new Blob([data], { type: mime }) : new Blob([data], { type: mime });
      const url = URL.createObjectURL(blob);
      const audio = new Audio(url);
      const cleanup = () => {
        URL.revokeObjectURL(url);
        playNextAudio();
      };
      audio.onended = cleanup;
      audio.onerror = cleanup;
      audio.play().catch(cleanup);
    }

    // Build a WAV container around raw PCM16 mono data.
    function pcm16ToWav(pcmBuffer, sampleRate = 16000) {
      const bytes = new Uint8Array(pcmBuffer);
      const buffer = new ArrayBuffer(44 + bytes.length);
      const view = new DataView(buffer);
      const writeString = (offset, str) => {
        for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
      };
      writeString(0, "RIFF");
      view.setUint32(4, 36 + bytes.length, true);
      writeString(8, "WAVE");
      writeString(12, "fmt ");
      view.setUint32(16, 16, true); // PCM chunk size
      view.setUint16(20, 1, true); // PCM format
      view.setUint16(22, 1, true); // mono
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true); // byte rate (16-bit mono)
      view.setUint16(32, 2, true); // block align
      view.setUint16(34, 16, true); // bits per sample
      writeString(36, "data");
      view.setUint32(40, bytes.length, true);
      new Uint8Array(buffer).set(bytes, 44);
      return buffer;
    }

    function appendTranscript(role, text) {
      const existing = transcriptLog.textContent || "";
      const line = `${role}: ${text}`;
      transcriptLog.textContent = existing ? `${existing}\n${line}` : line;
      conversationHistory.push({ role, message: text });

      // Auto-save after agent turns to persist progress
      if (role === "agent" && respondentId) {
        fetch(`${API_BASE}/api/conversation/autosave`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ respondent_id: respondentId, transcript: conversationHistory }),
        }).catch((err) => console.error("Auto-save failed:", err));
      }
    }

    function floatTo16BitPCM(float32Array) {
      const buffer = new ArrayBuffer(float32Array.length * 2);
      const view = new DataView(buffer);
      let offset = 0;
      for (let i = 0; i < float32Array.length; i++, offset += 2) {
        let s = Math.max(-1, Math.min(1, float32Array[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
      }
      return new Uint8Array(buffer);
    }
  </script>
</body>
</html>
